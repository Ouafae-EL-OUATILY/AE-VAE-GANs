{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1-D4HtAhCVN",
        "outputId": "a82ecfae-f9c3-4b1d-8f11-a7d02581250d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/10] Batch 100/1563 Discriminator Loss: 0.5082 Generator Loss: 1.1477\n",
            "Epoch [1/10] Batch 200/1563 Discriminator Loss: 0.6927 Generator Loss: 0.8794\n",
            "Epoch [1/10] Batch 300/1563 Discriminator Loss: 0.6862 Generator Loss: 1.0297\n",
            "Epoch [1/10] Batch 400/1563 Discriminator Loss: 0.5986 Generator Loss: 1.1756\n",
            "Epoch [1/10] Batch 500/1563 Discriminator Loss: 0.6212 Generator Loss: 1.1365\n",
            "Epoch [1/10] Batch 600/1563 Discriminator Loss: 0.7207 Generator Loss: 0.7874\n",
            "Epoch [1/10] Batch 700/1563 Discriminator Loss: 0.6555 Generator Loss: 0.9291\n",
            "Epoch [1/10] Batch 800/1563 Discriminator Loss: 0.6734 Generator Loss: 0.9019\n",
            "Epoch [1/10] Batch 900/1563 Discriminator Loss: 0.5815 Generator Loss: 1.0152\n",
            "Epoch [1/10] Batch 1000/1563 Discriminator Loss: 0.7481 Generator Loss: 0.8303\n",
            "Epoch [1/10] Batch 1100/1563 Discriminator Loss: 0.4935 Generator Loss: 1.2252\n",
            "Epoch [1/10] Batch 1200/1563 Discriminator Loss: 0.7237 Generator Loss: 0.9293\n",
            "Epoch [1/10] Batch 1300/1563 Discriminator Loss: 0.7625 Generator Loss: 0.8211\n",
            "Epoch [1/10] Batch 1400/1563 Discriminator Loss: 0.7563 Generator Loss: 0.8934\n",
            "Epoch [1/10] Batch 1500/1563 Discriminator Loss: 0.7729 Generator Loss: 0.9767\n",
            "Epoch [2/10] Batch 100/1563 Discriminator Loss: 0.6843 Generator Loss: 0.7922\n",
            "Epoch [2/10] Batch 200/1563 Discriminator Loss: 0.7523 Generator Loss: 0.9295\n",
            "Epoch [2/10] Batch 300/1563 Discriminator Loss: 0.7503 Generator Loss: 0.8348\n",
            "Epoch [2/10] Batch 400/1563 Discriminator Loss: 0.6026 Generator Loss: 0.9910\n",
            "Epoch [2/10] Batch 500/1563 Discriminator Loss: 0.6643 Generator Loss: 1.0516\n",
            "Epoch [2/10] Batch 600/1563 Discriminator Loss: 0.6659 Generator Loss: 0.7341\n",
            "Epoch [2/10] Batch 700/1563 Discriminator Loss: 0.5918 Generator Loss: 1.1570\n",
            "Epoch [2/10] Batch 800/1563 Discriminator Loss: 0.7742 Generator Loss: 0.6988\n",
            "Epoch [2/10] Batch 900/1563 Discriminator Loss: 0.6742 Generator Loss: 0.9036\n",
            "Epoch [2/10] Batch 1000/1563 Discriminator Loss: 0.6588 Generator Loss: 1.1231\n",
            "Epoch [2/10] Batch 1100/1563 Discriminator Loss: 0.5821 Generator Loss: 1.2368\n",
            "Epoch [2/10] Batch 1200/1563 Discriminator Loss: 0.4752 Generator Loss: 1.0657\n",
            "Epoch [2/10] Batch 1300/1563 Discriminator Loss: 0.5740 Generator Loss: 1.0244\n",
            "Epoch [2/10] Batch 1400/1563 Discriminator Loss: 0.6040 Generator Loss: 1.0220\n",
            "Epoch [2/10] Batch 1500/1563 Discriminator Loss: 0.6427 Generator Loss: 1.0970\n",
            "Epoch [3/10] Batch 100/1563 Discriminator Loss: 0.5356 Generator Loss: 1.1803\n",
            "Epoch [3/10] Batch 200/1563 Discriminator Loss: 0.6485 Generator Loss: 1.2309\n",
            "Epoch [3/10] Batch 300/1563 Discriminator Loss: 0.5923 Generator Loss: 0.8578\n",
            "Epoch [3/10] Batch 400/1563 Discriminator Loss: 0.6491 Generator Loss: 1.0452\n",
            "Epoch [3/10] Batch 500/1563 Discriminator Loss: 0.5536 Generator Loss: 1.0369\n",
            "Epoch [3/10] Batch 600/1563 Discriminator Loss: 0.5988 Generator Loss: 1.1523\n",
            "Epoch [3/10] Batch 700/1563 Discriminator Loss: 0.3760 Generator Loss: 1.2086\n",
            "Epoch [3/10] Batch 800/1563 Discriminator Loss: 0.5973 Generator Loss: 0.9885\n",
            "Epoch [3/10] Batch 900/1563 Discriminator Loss: 0.5751 Generator Loss: 1.2375\n",
            "Epoch [3/10] Batch 1000/1563 Discriminator Loss: 0.5067 Generator Loss: 1.1820\n",
            "Epoch [3/10] Batch 1100/1563 Discriminator Loss: 0.5750 Generator Loss: 1.3096\n",
            "Epoch [3/10] Batch 1200/1563 Discriminator Loss: 0.5330 Generator Loss: 1.0397\n",
            "Epoch [3/10] Batch 1300/1563 Discriminator Loss: 0.6407 Generator Loss: 1.2459\n",
            "Epoch [3/10] Batch 1400/1563 Discriminator Loss: 0.5219 Generator Loss: 0.8456\n",
            "Epoch [3/10] Batch 1500/1563 Discriminator Loss: 0.7268 Generator Loss: 0.9846\n",
            "Epoch [4/10] Batch 100/1563 Discriminator Loss: 0.6236 Generator Loss: 1.0952\n",
            "Epoch [4/10] Batch 200/1563 Discriminator Loss: 0.6225 Generator Loss: 1.1046\n",
            "Epoch [4/10] Batch 300/1563 Discriminator Loss: 0.6229 Generator Loss: 1.1824\n",
            "Epoch [4/10] Batch 400/1563 Discriminator Loss: 0.6643 Generator Loss: 1.0611\n",
            "Epoch [4/10] Batch 500/1563 Discriminator Loss: 0.7504 Generator Loss: 1.2384\n",
            "Epoch [4/10] Batch 600/1563 Discriminator Loss: 0.5294 Generator Loss: 1.4423\n",
            "Epoch [4/10] Batch 700/1563 Discriminator Loss: 0.7801 Generator Loss: 0.9566\n",
            "Epoch [4/10] Batch 800/1563 Discriminator Loss: 0.5592 Generator Loss: 1.0495\n",
            "Epoch [4/10] Batch 900/1563 Discriminator Loss: 0.7226 Generator Loss: 0.8877\n",
            "Epoch [4/10] Batch 1000/1563 Discriminator Loss: 0.6420 Generator Loss: 1.0941\n",
            "Epoch [4/10] Batch 1100/1563 Discriminator Loss: 0.5444 Generator Loss: 1.3297\n",
            "Epoch [4/10] Batch 1200/1563 Discriminator Loss: 0.7304 Generator Loss: 1.3447\n",
            "Epoch [4/10] Batch 1300/1563 Discriminator Loss: 0.7191 Generator Loss: 1.2405\n",
            "Epoch [4/10] Batch 1400/1563 Discriminator Loss: 0.6349 Generator Loss: 1.2933\n",
            "Epoch [4/10] Batch 1500/1563 Discriminator Loss: 0.4946 Generator Loss: 1.3737\n",
            "Epoch [5/10] Batch 100/1563 Discriminator Loss: 0.6514 Generator Loss: 0.9875\n",
            "Epoch [5/10] Batch 200/1563 Discriminator Loss: 0.5782 Generator Loss: 0.9730\n",
            "Epoch [5/10] Batch 300/1563 Discriminator Loss: 0.5501 Generator Loss: 1.5972\n",
            "Epoch [5/10] Batch 400/1563 Discriminator Loss: 0.6856 Generator Loss: 0.9320\n",
            "Epoch [5/10] Batch 500/1563 Discriminator Loss: 0.4368 Generator Loss: 0.9196\n",
            "Epoch [5/10] Batch 600/1563 Discriminator Loss: 0.5253 Generator Loss: 1.1793\n",
            "Epoch [5/10] Batch 700/1563 Discriminator Loss: 0.5686 Generator Loss: 1.6707\n",
            "Epoch [5/10] Batch 800/1563 Discriminator Loss: 0.5808 Generator Loss: 1.1782\n",
            "Epoch [5/10] Batch 900/1563 Discriminator Loss: 0.7754 Generator Loss: 0.8495\n",
            "Epoch [5/10] Batch 1000/1563 Discriminator Loss: 0.5273 Generator Loss: 1.5617\n",
            "Epoch [5/10] Batch 1100/1563 Discriminator Loss: 0.4883 Generator Loss: 1.3541\n",
            "Epoch [5/10] Batch 1200/1563 Discriminator Loss: 0.5299 Generator Loss: 1.3354\n",
            "Epoch [5/10] Batch 1300/1563 Discriminator Loss: 0.7520 Generator Loss: 1.2887\n",
            "Epoch [5/10] Batch 1400/1563 Discriminator Loss: 0.6637 Generator Loss: 1.0412\n",
            "Epoch [5/10] Batch 1500/1563 Discriminator Loss: 0.4716 Generator Loss: 1.3912\n",
            "Epoch [6/10] Batch 100/1563 Discriminator Loss: 0.7875 Generator Loss: 1.8903\n",
            "Epoch [6/10] Batch 200/1563 Discriminator Loss: 0.6014 Generator Loss: 1.2960\n",
            "Epoch [6/10] Batch 300/1563 Discriminator Loss: 0.3848 Generator Loss: 1.6972\n",
            "Epoch [6/10] Batch 400/1563 Discriminator Loss: 0.4878 Generator Loss: 1.3948\n",
            "Epoch [6/10] Batch 500/1563 Discriminator Loss: 0.7167 Generator Loss: 0.7895\n",
            "Epoch [6/10] Batch 600/1563 Discriminator Loss: 0.4485 Generator Loss: 1.2055\n",
            "Epoch [6/10] Batch 700/1563 Discriminator Loss: 0.5103 Generator Loss: 1.7942\n",
            "Epoch [6/10] Batch 800/1563 Discriminator Loss: 0.4569 Generator Loss: 0.9726\n",
            "Epoch [6/10] Batch 900/1563 Discriminator Loss: 0.5983 Generator Loss: 0.9460\n",
            "Epoch [6/10] Batch 1000/1563 Discriminator Loss: 0.4474 Generator Loss: 1.3084\n",
            "Epoch [6/10] Batch 1100/1563 Discriminator Loss: 0.6220 Generator Loss: 1.4487\n",
            "Epoch [6/10] Batch 1200/1563 Discriminator Loss: 0.5630 Generator Loss: 1.7046\n",
            "Epoch [6/10] Batch 1300/1563 Discriminator Loss: 0.4764 Generator Loss: 1.3585\n",
            "Epoch [6/10] Batch 1400/1563 Discriminator Loss: 0.5065 Generator Loss: 1.4044\n",
            "Epoch [6/10] Batch 1500/1563 Discriminator Loss: 0.7480 Generator Loss: 0.6914\n",
            "Epoch [7/10] Batch 100/1563 Discriminator Loss: 0.3483 Generator Loss: 1.4812\n",
            "Epoch [7/10] Batch 200/1563 Discriminator Loss: 0.5516 Generator Loss: 0.9086\n",
            "Epoch [7/10] Batch 300/1563 Discriminator Loss: 0.4300 Generator Loss: 1.4807\n",
            "Epoch [7/10] Batch 400/1563 Discriminator Loss: 0.6361 Generator Loss: 0.7694\n",
            "Epoch [7/10] Batch 500/1563 Discriminator Loss: 0.5377 Generator Loss: 1.5692\n",
            "Epoch [7/10] Batch 600/1563 Discriminator Loss: 0.7031 Generator Loss: 0.6102\n",
            "Epoch [7/10] Batch 700/1563 Discriminator Loss: 0.9771 Generator Loss: 1.1562\n",
            "Epoch [7/10] Batch 800/1563 Discriminator Loss: 0.7270 Generator Loss: 1.5989\n",
            "Epoch [7/10] Batch 900/1563 Discriminator Loss: 0.5701 Generator Loss: 0.7918\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='/content/drive/MyDrive/Abstract', train=True, download=True, transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "num_epochs = 10\n",
        "\n",
        "# Define the generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128 * 8 * 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 8, 8)),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128, momentum=0.78),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64, momentum=0.78),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        return img\n",
        "\n",
        "# Define the discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ZeroPad2d((0, 1, 0, 1)),\n",
        "            nn.BatchNorm2d(64, momentum=0.82),\n",
        "            nn.LeakyReLU(0.25),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128, momentum=0.82),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256, momentum=0.8),\n",
        "            nn.LeakyReLU(0.25),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 5 * 5, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        validity = self.model(img)\n",
        "        return validity.view(img.size(0), -1)  # Flatten the output\n",
        "\n",
        "# Instantiate the generator and discriminator\n",
        "generator = Generator(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        real_images = batch[0].to(device)\n",
        "        valid = torch.ones(real_images.size(0), 1, device=device)\n",
        "        fake = torch.zeros(real_images.size(0), 1, device=device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        z = torch.randn(real_images.size(0), latent_dim, device=device)\n",
        "        fake_images = generator(z)\n",
        "        real_loss = adversarial_loss(discriminator(real_images), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(fake_images.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        gen_images = generator(z)\n",
        "        g_loss = adversarial_loss(discriminator(gen_images), valid)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Progress Monitoring\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch {i+1}/{len(dataloader)} \"\n",
        "                  f\"Discriminator Loss: {d_loss.item():.4f} Generator Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Save generated images for every epoch\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(16, latent_dim, device=device)\n",
        "            generated = generator(z).detach().cpu()\n",
        "            grid = torchvision.utils.make_grid(generated, nrow=4, normalize=True)\n",
        "            plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.utils\n",
        "\n",
        "# Generate new data\n",
        "with torch.no_grad():\n",
        "    num_samples = 10  # You can adjust this based on your preference\n",
        "    z = torch.randn(num_samples, latent_dim, device=device)\n",
        "    generated_images = generator(z).detach().cpu()\n",
        "\n",
        "# Visualize the original and generated images\n",
        "real_images_batch = next(iter(dataloader))[0][:num_samples].cpu()\n",
        "\n",
        "# Plot the original and generated images\n",
        "fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 2, 4))\n",
        "\n",
        "for i in range(num_samples):\n",
        "    axes[0, i].imshow(np.transpose(real_images_batch[i], (1, 2, 0)))\n",
        "    axes[0, i].axis('off')\n",
        "    axes[0, i].set_title('Original')\n",
        "\n",
        "    axes[1, i].imshow(np.transpose(generated_images[i], (1, 2, 0)))\n",
        "    axes[1, i].axis('off')\n",
        "    axes[1, i].set_title('Generated')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oeReJTE1DhUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}